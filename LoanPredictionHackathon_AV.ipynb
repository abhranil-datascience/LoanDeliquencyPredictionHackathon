{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LoanPredictionHackathon_AV.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhranil-datascience/LoanDeliquencyPredictionHackathon/blob/master/LoanPredictionHackathon_AV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GATsvk5qN-yU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Public LeaderBoard Score: 0.33\n",
        "######################### Suppress Warnings #######################################\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "############################## Mount Drive ########################################\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "############################## Change Directory ###################################\n",
        "\n",
        "import os\n",
        "os.chdir('/content/gdrive/My Drive/AnalyticsVidhya/MLHackathon2019')\n",
        "\n",
        "################# Root Import Statements #####################\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import Imputer\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder,OneHotEncoder,RobustScaler, StandardScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from keras.models import Sequential,load_model\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras import backend as K\n",
        "from sklearn.metrics import f1_score\n",
        "from keras.optimizers import Adam\n",
        "import random\n",
        "#from numpy.random import seed\n",
        "#seed(1)\n",
        "\n",
        "################# Declare Functions ##########################\n",
        "def recall_m(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\n",
        "################# Declare Constants ##########################\n",
        "\n",
        "TrainingDatasetPath=\"Dataset/LoanDeliquencyTrain.csv\"\n",
        "TestDatasetPath=\"Dataset/LoanDeliquencyTest.csv\"\n",
        "BestClassifierModel=\"BestModel/best_model.hdf5\"\n",
        "SaveBestModel=ModelCheckpoint(filepath=BestClassifierModel,monitor='val_f1_m', verbose=1, save_best_only=True, mode='max')\n",
        "EarlyStopping=EarlyStopping(monitor='val_f1_m', min_delta=0.001, patience=12, verbose=1, mode='max', baseline=None, restore_best_weights=True)\n",
        "\n",
        "###################################### Description #####################################################################################\n",
        "\n",
        "#Index 0 : source || Index 1: financial_institution || Index 2: interest_rate || Index 3: unpaid_principal_bal || Index 4: loan_term || Index 5 : origination_date ||\n",
        "\n",
        "#Index 6 : first_payment_date || Index 7: loan_to_value || Index 8: number_of_borrowers || Index 9: debt_to_income_ratio || Index 10: borrower_credit_score ||  \n",
        "\n",
        "#Index 11 : loan_purpose || Index 12: insurance_percent || Index 13: co-borrower_credit_score || Index 14: insurance_type || \n",
        "\n",
        "#Index 15 : m1 || Index 16: m2 || Index 17: m3 || Index 18: m4 || Index 19: m5 || Index 20 : m6 || Index 21: m7 || Index 22: m8 || Index 23: m9 || Index 24 : m10 || \n",
        "\n",
        "#Index 25 : m11 || Index 26: m12 || Index 27: m13(Target Variable)\n",
        "\n",
        "#####################################################################################################################################################################\n",
        "\n",
        "#################################################################### Functions ######################################################################################\n",
        "# 1. Format Date since Train and Test have different date format\n",
        "def FormatDate(TrainDataset,TestDataset):\n",
        "  num_rows_in_train=TrainDataset.shape[0]\n",
        "  num_rows_in_test=TestDataset.shape[0]\n",
        "  for row_num in range(0,num_rows_in_train):\n",
        "    curr_orig_date=TrainDataset[row_num][5]\n",
        "    curr_pay_date=TrainDataset[row_num][6]\n",
        "    curr_orig_date_token=curr_orig_date.split('-')\n",
        "    curr_orig_month=curr_orig_date_token[1]\n",
        "    curr_pay_date_token=curr_pay_date.split('/')\n",
        "    curr_pay_month=curr_pay_date_token[0]\n",
        "    if curr_orig_month==\"01\":\n",
        "      TrainDataset[row_num][5]=\"J\"\n",
        "    elif curr_orig_month==\"02\":\n",
        "      TrainDataset[row_num][5]=\"F\"\n",
        "    elif curr_orig_month==\"03\":\n",
        "      TrainDataset[row_num][5]=\"M\"\n",
        "    else:\n",
        "      TrainDataset[row_num][5]=\"NA\"\n",
        "    if curr_pay_month==\"02\":\n",
        "      TrainDataset[row_num][6]=\"F\"\n",
        "    elif curr_pay_month==\"03\":\n",
        "      TrainDataset[row_num][6]=\"M\"\n",
        "    elif curr_pay_month==\"04\":\n",
        "      TrainDataset[row_num][6]=\"A\"\n",
        "    elif curr_pay_month==\"05\":\n",
        "      TrainDataset[row_num][6]=\"MA\"\n",
        "    else:\n",
        "      TrainDataset[row_num][6]=\"NA\"\n",
        "  for row_num in range(0,num_rows_in_test):\n",
        "    curr_orig_date=TestDataset[row_num][5]\n",
        "    curr_pay_date=TestDataset[row_num][6]\n",
        "    curr_orig_date_token=curr_orig_date.split('/')\n",
        "    curr_orig_month=curr_orig_date_token[1]\n",
        "    curr_pay_date_token=curr_pay_date.split('-')\n",
        "    curr_pay_month=curr_pay_date_token[0]\n",
        "    if curr_orig_month==\"01\":\n",
        "      TestDataset[row_num][5]=\"J\"\n",
        "    elif curr_orig_month==\"02\":\n",
        "      TestDataset[row_num][5]=\"F\"\n",
        "    elif curr_orig_month==\"03\":\n",
        "      TestDataset[row_num][5]=\"M\"\n",
        "    else:\n",
        "      TestDataset[row_num][5]=\"NA\"\n",
        "    if curr_pay_month==\"Feb\":\n",
        "      TestDataset[row_num][6]=\"F\"\n",
        "    elif curr_pay_month==\"Mar\":\n",
        "      TestDataset[row_num][6]=\"M\"\n",
        "    elif curr_pay_month==\"Apr\":\n",
        "      TestDataset[row_num][6]=\"A\"\n",
        "    elif curr_pay_month==\"May\":\n",
        "      TestDataset[row_num][6]=\"MA\"\n",
        "    else:\n",
        "      TestDataset[row_num][6]=\"NA\"\n",
        "  return TrainDataset,TestDataset\n",
        "\n",
        "# 2. Function to deal with invalid values\n",
        "def DealWithMissingValues(dataset):\n",
        "  imputer=Imputer(missing_values=0,strategy='mean', axis=0)\n",
        "  dataset[:,10:11]=imputer.fit_transform(dataset[:,10:11])\n",
        "  for row_num in range(0,dataset.shape[0]):\n",
        "    if dataset[row_num][8]==1:\n",
        "      if dataset[row_num][13] != 0:\n",
        "        dataset[row_num][13] = 0\n",
        "  return dataset\n",
        "  \n",
        "# 3. Understand pattern in M1 to M12\n",
        "def UnderstandPatternFromM1toM12(m1_to_m12):\n",
        "  num_of_rows=m1_to_m12.shape[0]\n",
        "  num_of_columns=m1_to_m12.shape[1]\n",
        "  sum_of_defaults=[]\n",
        "  regular_defaulter=[]\n",
        "  odd_defaulter=[]\n",
        "  last_month_defaulter=[]\n",
        "  last_two_month_defaulter=[]\n",
        "  num_of_defaults_in_last_two_month=[]\n",
        "  last_three_month_defaulter=[]\n",
        "  num_of_defaults_in_last_three_month=[]\n",
        "  num_of_deliquency_till_m13=[]\n",
        "  ProbabilityToDefaultInWhole=[]\n",
        "  ProbabilityToDefaultInSixMon=[]\n",
        "  ProbabilityToDefaultInThreeMon=[]\n",
        "  for row_num in range(0,num_of_rows):\n",
        "    ################## Calculating Sum of Defaults ##########################\n",
        "    sum_of_default_in_current_row=0\n",
        "    curr_prob_whole=0\n",
        "    curr_six_mon_prob=0\n",
        "    curr_three_mon_prob=0\n",
        "    for col_num in range(0,num_of_columns):\n",
        "      sum_of_default_in_current_row=sum_of_default_in_current_row+m1_to_m12[row_num][col_num]\n",
        "      if m1_to_m12[row_num][col_num] != 0:\n",
        "        curr_prob_whole=curr_prob_whole+1\n",
        "      if col_num > 5:\n",
        "        if m1_to_m12[row_num][col_num] != 0:\n",
        "          curr_six_mon_prob=curr_six_mon_prob+1\n",
        "      if col_num > 8:\n",
        "        if m1_to_m12[row_num][col_num] != 0:\n",
        "          curr_three_mon_prob=curr_three_mon_prob+1\n",
        "    curr_prob_whole=curr_prob_whole/12\n",
        "    curr_six_mon_prob=curr_six_mon_prob/6\n",
        "    curr_three_mon_prob=curr_three_mon_prob/3\n",
        "    ProbabilityToDefaultInWhole.append(curr_prob_whole)\n",
        "    ProbabilityToDefaultInSixMon.append(curr_six_mon_prob)\n",
        "    ProbabilityToDefaultInThreeMon.append(curr_three_mon_prob)\n",
        "    sum_of_defaults.append(sum_of_default_in_current_row)\n",
        "    ### Calculating Regular defaulter by checking if he defaulted on every 2 months ###\n",
        "    m1_m2 =  m1_to_m12[row_num][0] + m1_to_m12[row_num][1]\n",
        "    m3_m4 =  m1_to_m12[row_num][2] + m1_to_m12[row_num][3]\n",
        "    m5_m6 =  m1_to_m12[row_num][4] + m1_to_m12[row_num][5]\n",
        "    m7_m8 =  m1_to_m12[row_num][6] + m1_to_m12[row_num][7]\n",
        "    m9_m10 =  m1_to_m12[row_num][8] + m1_to_m12[row_num][9]\n",
        "    m11_m12 =  m1_to_m12[row_num][10] + m1_to_m12[row_num][11]\n",
        "    if m1_m2 > 0: \n",
        "      m1_m2=1\n",
        "    else:\n",
        "      m1_m2=0\n",
        "    if m3_m4 > 0: \n",
        "      m3_m4=1\n",
        "    else:\n",
        "      m3_m4=0 \n",
        "    if m5_m6 > 0: \n",
        "      m5_m6=1\n",
        "    else:\n",
        "      m5_m6=0 \n",
        "    if m7_m8 > 0: \n",
        "      m7_m8=1\n",
        "    else:\n",
        "      m7_m8=0\n",
        "    if m9_m10 > 0: \n",
        "      m9_m10=1\n",
        "    else:\n",
        "      m9_m10=0\n",
        "    if m11_m12 > 0: \n",
        "      m11_m12=1\n",
        "    else:\n",
        "      m11_m12=0\n",
        "    if m1_m2+m3_m4+m5_m6+m7_m8+m9_m10+m11_m12 > 2:\n",
        "      regular_defaulter.append(\"Y\")\n",
        "    else:\n",
        "      regular_defaulter.append(\"N\")\n",
        "    ########### Calculating Odd Defaulter ###############\n",
        "    m1=0\n",
        "    m3=0\n",
        "    m5=0\n",
        "    m7=0\n",
        "    m9=0\n",
        "    m11=0\n",
        "    if m1_to_m12[row_num][0]!=0:\n",
        "      m1=1\n",
        "    if m1_to_m12[row_num][2]!=0:\n",
        "      m3=1\n",
        "    if m1_to_m12[row_num][4]!=0:\n",
        "      m5=1\n",
        "    if m1_to_m12[row_num][6]!=0:\n",
        "      m7=1\n",
        "    if m1_to_m12[row_num][8]!=0:\n",
        "      m9=1\n",
        "    if m1_to_m12[row_num][10]!=0:\n",
        "      m11=1\n",
        "    if (m1+m3+m5+m7+m9+m11)/6 > 0.5:\n",
        "      odd_defaulter.append(\"Y\")\n",
        "    else:\n",
        "      odd_defaulter.append(\"N\")\n",
        "    ############ Calculating last Month Defaulter #############\n",
        "    if m1_to_m12[row_num][11]>0:\n",
        "      last_month_defaulter.append(\"Y\")\n",
        "    else:\n",
        "      last_month_defaulter.append(\"N\")\n",
        "    ########## Checking if defaulted in last 2 months ##########\n",
        "    if m1_to_m12[row_num][10]>0 and m1_to_m12[row_num][11]>0:\n",
        "      last_two_month_defaulter.append(\"Y\")\n",
        "    else:\n",
        "      last_two_month_defaulter.append(\"N\")\n",
        "    ###### Checking num of defaults in last 2 months ###########\n",
        "    num_of_defaults_in_last_two_month.append(m1_to_m12[row_num][10]+m1_to_m12[row_num][11])\n",
        "    ########## Checking if defaulted in last 3 months ##########\n",
        "    if m1_to_m12[row_num][9]>0 and m1_to_m12[row_num][10]>0 and m1_to_m12[row_num][11]>0:\n",
        "      last_three_month_defaulter.append(\"Y\")\n",
        "    else:\n",
        "      last_three_month_defaulter.append(\"N\")\n",
        "    ###### Checking num of defaults in last 3 months ###########\n",
        "    num_of_defaults_in_last_three_month.append(m1_to_m12[row_num][9]+m1_to_m12[row_num][10]+m1_to_m12[row_num][11])\n",
        "    ###### Number of deliquency till m13 ###############\n",
        "    num_of_del_still_pending=0\n",
        "    if m1_to_m12[row_num][0] >= 12:\n",
        "      num_of_del_still_pending=num_of_del_still_pending+1\n",
        "    if m1_to_m12[row_num][1] >= 11:\n",
        "      num_of_del_still_pending=num_of_del_still_pending+1\n",
        "    if m1_to_m12[row_num][2] >= 10:\n",
        "      num_of_del_still_pending=num_of_del_still_pending+1\n",
        "    if m1_to_m12[row_num][3] >= 9:\n",
        "      num_of_del_still_pending=num_of_del_still_pending+1\n",
        "    if m1_to_m12[row_num][4] >= 8:\n",
        "      num_of_del_still_pending=num_of_del_still_pending+1\n",
        "    if m1_to_m12[row_num][5] >= 7:\n",
        "      num_of_del_still_pending=num_of_del_still_pending+1\n",
        "    if m1_to_m12[row_num][6] >= 6:\n",
        "      num_of_del_still_pending=num_of_del_still_pending+1\n",
        "    if m1_to_m12[row_num][7] >= 5:\n",
        "      num_of_del_still_pending=num_of_del_still_pending+1\n",
        "    if m1_to_m12[row_num][8] >= 4:\n",
        "      num_of_del_still_pending=num_of_del_still_pending+1\n",
        "    if m1_to_m12[row_num][9] >= 3:\n",
        "      num_of_del_still_pending=num_of_del_still_pending+1\n",
        "    if m1_to_m12[row_num][10] >= 2:\n",
        "      num_of_del_still_pending=num_of_del_still_pending+1\n",
        "    if m1_to_m12[row_num][11] >= 1:\n",
        "      num_of_del_still_pending=num_of_del_still_pending+1\n",
        "    num_of_deliquency_till_m13.append(num_of_del_still_pending)\n",
        "  \n",
        "  return sum_of_defaults,regular_defaulter,odd_defaulter,last_month_defaulter,last_two_month_defaulter,num_of_defaults_in_last_two_month,last_three_month_defaulter,num_of_defaults_in_last_three_month,num_of_deliquency_till_m13,ProbabilityToDefaultInWhole,ProbabilityToDefaultInSixMon,ProbabilityToDefaultInThreeMon\n",
        "\n",
        "# 4. Segment Credit Score\n",
        "def SegmentScreditScore(dataset,credit_score_col_num):\n",
        "  Credit_Score=[]\n",
        "  for row_num in range(0,dataset.shape[0]):\n",
        "    curr_score=dataset[row_num][credit_score_col_num]\n",
        "    if curr_score > 299 and curr_score < 580:\n",
        "      Credit_Score.append(\"VP\")\n",
        "    elif curr_score > 579 and curr_score < 670:\n",
        "      Credit_Score.append(\"P\")\n",
        "    elif curr_score > 669 and curr_score < 740:\n",
        "      Credit_Score.append(\"A\")\n",
        "    elif curr_score > 739 and curr_score < 800:\n",
        "      Credit_Score.append(\"G\")\n",
        "    elif curr_score > 799 and curr_score <= 850:\n",
        "      Credit_Score.append(\"VG\")\n",
        "    else:\n",
        "      Credit_Score.append(\"NA\")\n",
        "  return Credit_Score\n",
        "\n",
        "# 5. Parse Origin Date\n",
        "def ParseOriginandfirstpaymentDate(dataset,origin_column_index,payment_column_index):\n",
        "  OriginMonth=[]\n",
        "  PaymentMonth=[]\n",
        "  Interval=[]\n",
        "  Recurrence=[]\n",
        "  EMIFrequency=[]\n",
        "  for row_num in range(0,dataset.shape[0]):\n",
        "    orig_month=dataset[row_num][origin_column_index]\n",
        "    pay_month=dataset[row_num][payment_column_index]\n",
        "    OriginMonth.append(orig_month)\n",
        "    PaymentMonth.append(pay_month)\n",
        "    if orig_month==\"J\" and pay_month==\"F\":\n",
        "      Interval.append(\"A\")\n",
        "    elif orig_month==\"J\" and pay_month==\"M\":\n",
        "      Interval.append(\"B\")\n",
        "    elif orig_month==\"F\" and pay_month==\"A\":\n",
        "      Interval.append(\"C\")\n",
        "    elif (orig_month==\"F\" and pay_month==\"M\") or (orig_month==\"M\" and pay_month==\"A\"):\n",
        "      Interval.append(\"D\")\n",
        "    else:\n",
        "      Interval.append(\"E\")\n",
        "    if (orig_month==\"J\" and pay_month==\"F\") or (orig_month==\"F\" and pay_month==\"M\") or (orig_month==\"M\" and pay_month==\"A\"):\n",
        "      Recurrence.append(\"A\")\n",
        "      EMIFrequency.append(1)\n",
        "    elif (orig_month==\"J\" and pay_month==\"M\") or (orig_month==\"F\" and pay_month==\"A\") or (orig_month==\"M\" and pay_month==\"MA\"):\n",
        "      Recurrence.append(\"B\")\n",
        "      EMIFrequency.append(2)\n",
        "    else:\n",
        "      Recurrence.append(\"C\")\n",
        "      EMIFrequency.append(3)\n",
        "  return OriginMonth,PaymentMonth,Interval,Recurrence,EMIFrequency\n",
        "    \n",
        "# 6. Parse Loan Term and Principal\n",
        "def ParseLoanTermandPrincipal(dataset,interest_rate,loan_term_index,unpaid_principal_index,borrower_count,EMIFrequency,insurance_percent,debt_to_income_ratio):\n",
        "  LoanTerm=[]\n",
        "  UnpaidPrincipal=[]\n",
        "  PtoTRatio=[]\n",
        "  InterestPending=[]\n",
        "  EMI=[]\n",
        "  EMIPerBorrower=[]\n",
        "  UnpaidPrincipalNotInsured=[]\n",
        "  Potential=[]\n",
        "  LoanSize=[]\n",
        "  for row_num in range(0,dataset.shape[0]):\n",
        "    curr_loan_term=dataset[row_num][loan_term_index]\n",
        "    curr_unpaid_principal=dataset[row_num][unpaid_principal_index]\n",
        "    curr_interest_rate=dataset[row_num][interest_rate]\n",
        "    curr_PtoT_ratio=curr_unpaid_principal/curr_loan_term\n",
        "    curr_interest_pending=curr_loan_term*curr_unpaid_principal*curr_interest_rate\n",
        "    curr_borrower_count=borrower_count[row_num][0]\n",
        "    if curr_borrower_count==0:\n",
        "      curr_borrower_count=1\n",
        "    else:\n",
        "      curr_borrower_count=2\n",
        "    curr_EMI_frequency=EMIFrequency[row_num][0]\n",
        "    curr_insurance_percent=insurance_percent[row_num][0]\n",
        "    curr_debt_to_income=debt_to_income_ratio[row_num][0]\n",
        "    LoanTerm.append(curr_loan_term)\n",
        "    UnpaidPrincipal.append(curr_unpaid_principal)\n",
        "    PtoTRatio.append(curr_PtoT_ratio)\n",
        "    InterestPending.append(curr_interest_pending)\n",
        "    per_month_principal=curr_unpaid_principal/curr_loan_term\n",
        "    curr_emi=(per_month_principal+per_month_principal*(100+curr_interest_rate))/curr_EMI_frequency\n",
        "    EMI.append(curr_emi)\n",
        "    curr_emi_per_borr=curr_emi/curr_borrower_count\n",
        "    EMIPerBorrower.append(curr_emi_per_borr)\n",
        "    curr_unpaid_principal_not_insured=curr_unpaid_principal-(curr_unpaid_principal*curr_insurance_percent/100)\n",
        "    UnpaidPrincipalNotInsured.append(curr_unpaid_principal_not_insured)\n",
        "    curr_potential=curr_unpaid_principal/(curr_loan_term*curr_debt_to_income)\n",
        "    Potential.append(curr_potential)\n",
        "    LoanSize.append(curr_unpaid_principal*(100+curr_interest_rate)/curr_loan_term)\n",
        "  return LoanTerm,UnpaidPrincipal,PtoTRatio,InterestPending,EMI,EMIPerBorrower,UnpaidPrincipalNotInsured,Potential,LoanSize\n",
        "\n",
        "\n",
        "## 7. OneHotEnoding Data\n",
        "def OneHotEncodeDataset(feature):\n",
        "  le=LabelEncoder()\n",
        "  oe=OneHotEncoder(categorical_features=[0])\n",
        "  feature[:,0]=le.fit_transform(feature[:,0])\n",
        "  featureOHE=oe.fit_transform(feature).toarray()\n",
        "  featureOHE=featureOHE[:,1:]\n",
        "  return featureOHE\n",
        "\n",
        "## 8. Preprocess Data\n",
        "def PreprocessDataset(dataset):\n",
        "  ########## Get M1 to M12 insights ########################\n",
        "  m1_to_m12=dataset[:,15:27]\n",
        "  sum_of_defaults,regular_defaulter,odd_defaulter,last_month_defaulter,last_two_month_defaulter,num_of_defaults_in_last_two_month,last_three_month_defaulter,num_of_defaults_in_last_three_month,num_of_deliquency_till_m13,ProbabilityToDefaultInWhole,ProbabilityToDefaultInSixMon,ProbabilityToDefaultInThreeMon=UnderstandPatternFromM1toM12(m1_to_m12)\n",
        "  m1=m1_to_m12=dataset[:,15:16]\n",
        "  m2=m1_to_m12=dataset[:,16:17]\n",
        "  m3=m1_to_m12=dataset[:,17:18]\n",
        "  m4=m1_to_m12=dataset[:,18:19]\n",
        "  m5=m1_to_m12=dataset[:,19:20]\n",
        "  m6=m1_to_m12=dataset[:,20:21]\n",
        "  m7=m1_to_m12=dataset[:,21:22]\n",
        "  m8=m1_to_m12=dataset[:,22:23]\n",
        "  m9=m1_to_m12=dataset[:,23:24]\n",
        "  m10=m1_to_m12=dataset[:,24:25]\n",
        "  m11=m1_to_m12=dataset[:,25:26]\n",
        "  m12=m1_to_m12=dataset[:,26:27]\n",
        "  sum_of_defaults=np.array(sum_of_defaults,dtype=object).reshape(-1,1)\n",
        "  regular_defaulter=np.array(regular_defaulter,dtype=object).reshape(-1,1)\n",
        "  odd_defaulter=np.array(odd_defaulter,dtype=object).reshape(-1,1)\n",
        "  last_month_defaulter=np.array(last_month_defaulter,dtype=object).reshape(-1,1)\n",
        "  last_two_month_defaulter=np.array(last_two_month_defaulter,dtype=object).reshape(-1,1)\n",
        "  num_of_defaults_in_last_two_month=np.array(num_of_defaults_in_last_two_month,dtype=object).reshape(-1,1)\n",
        "  last_three_month_defaulter=np.array(last_three_month_defaulter,dtype=object).reshape(-1,1)\n",
        "  num_of_defaults_in_last_three_month=np.array(num_of_defaults_in_last_three_month,dtype=object).reshape(-1,1)\n",
        "  num_of_deliquency_till_m13=np.array(num_of_deliquency_till_m13,dtype=object).reshape(-1,1)\n",
        "  ProbabilityToDefaultInWhole=np.array(ProbabilityToDefaultInWhole,dtype=object).reshape(-1,1)\n",
        "  ProbabilityToDefaultInSixMon=np.array(ProbabilityToDefaultInSixMon,dtype=object).reshape(-1,1)\n",
        "  ProbabilityToDefaultInThreeMon=np.array(ProbabilityToDefaultInThreeMon,dtype=object).reshape(-1,1)\n",
        "\n",
        "  ############## Insurance Type ####################\n",
        "  insurance_type=dataset[:,14:15]\n",
        "\n",
        "  ############## Co Borrower's Credit Score ####################\n",
        "  co_borrower_credit_score=SegmentScreditScore(dataset,13)\n",
        "  co_borrower_credit_score=np.array(co_borrower_credit_score,dtype=object).reshape(-1,1)\n",
        "\n",
        "  ############## Insurance Percent #############################\n",
        "  insurance_percent=dataset[:,12:13]\n",
        "\n",
        "  ############### Loan Purpose #########################\n",
        "  loan_purpose=dataset[:,11]\n",
        "  loan_purpose[loan_purpose==\"A23\"]=\"A\"\n",
        "  loan_purpose[loan_purpose==\"B12\"]=\"B\"\n",
        "  loan_purpose[loan_purpose==\"C86\"]=\"C\"\n",
        "  loan_purpose=loan_purpose.reshape(-1,1)\n",
        "\n",
        "  ################### Borrower's Credit Score ######################\n",
        "  borrower_credit_score_number=dataset[:,10:11]\n",
        "  borrower_credit_score=SegmentScreditScore(dataset,10)\n",
        "  borrower_credit_score=np.array(borrower_credit_score,dtype=object).reshape(-1,1)\n",
        "\n",
        "  ################### Debt to income ratio #########################\n",
        "  debt_to_income_ratio=dataset[:,9:10]\n",
        "\n",
        "  ################## Number Of Borrower #############################\n",
        "  num_of_borrower=dataset[:,8]\n",
        "  num_of_borrower[num_of_borrower==1]=0\n",
        "  num_of_borrower[num_of_borrower==2]=1\n",
        "  num_of_borrower=num_of_borrower.reshape(-1,1)\n",
        "\n",
        "  ################# Loan To Value #############################\n",
        "  loan_to_value=dataset[:,7:8]\n",
        "\n",
        "  ######### Origin, Payment and Interval Dates ################\n",
        "  OriginMonth,PaymentMonth,Interval,Recurrence,EMIFrequency=ParseOriginandfirstpaymentDate(dataset,5,6)\n",
        "  OriginMonth=np.array(OriginMonth,dtype=object).reshape(-1,1)\n",
        "  PaymentMonth=np.array(PaymentMonth,dtype=object).reshape(-1,1)\n",
        "  Interval=np.array(Interval,dtype=object).reshape(-1,1)\n",
        "  Recurrence=np.array(Recurrence,dtype=object).reshape(-1,1)\n",
        "  EMIFrequency=np.array(EMIFrequency,dtype=object).reshape(-1,1)\n",
        "\n",
        "  ##### Unpaid Principal, Loan Term and PtoT ratio ############\n",
        "  borrower_count=dataset[:,8:9]\n",
        "  LoanTerm,UnpaidPrincipal,PtoTRatio,InterestPending,EMI,EMIPerBorrower,UnpaidPrincipalNotInsured,Potential,LoanSize=ParseLoanTermandPrincipal(dataset,2,4,3,borrower_count,EMIFrequency,insurance_percent,debt_to_income_ratio)\n",
        "  LoanTerm=np.array(LoanTerm,dtype=object).reshape(-1,1)\n",
        "  UnpaidPrincipal=np.array(UnpaidPrincipal,dtype=object).reshape(-1,1)\n",
        "  PtoTRatio=np.array(PtoTRatio,dtype=object).reshape(-1,1)\n",
        "  InterestPending=np.array(InterestPending,dtype=object).reshape(-1,1)\n",
        "  EMI=np.array(EMI,dtype=object).reshape(-1,1)\n",
        "  EMIPerBorrower=np.array(EMIPerBorrower,dtype=object).reshape(-1,1)\n",
        "  UnpaidPrincipalNotInsured=np.array(UnpaidPrincipalNotInsured,dtype=object).reshape(-1,1)\n",
        "  Potential=np.array(Potential,dtype=object).reshape(-1,1)\n",
        "  LoanSize=np.array(LoanSize,dtype=object).reshape(-1,1)\n",
        "\n",
        "  ################# Interest Rate #############################\n",
        "  interest_rate=dataset[:,2:3]\n",
        "\n",
        "  ################ Financial Institution #######################\n",
        "  financial_institution=dataset[:,1:2]\n",
        "\n",
        "  ###################### Source #################################\n",
        "  source=dataset[:,0:1]\n",
        "  \n",
        "  DatasetPreProcessedNoOHE= np.concatenate((source,financial_institution,interest_rate,UnpaidPrincipal,LoanTerm,PtoTRatio,OriginMonth,PaymentMonth,Interval,Recurrence,\n",
        "                                            loan_to_value,num_of_borrower,debt_to_income_ratio,borrower_credit_score,loan_purpose,insurance_percent,co_borrower_credit_score,\n",
        "                                            insurance_type,m1,m2,m3,m4,m5,m6,m7,m8,m9,m10,m11,m12,sum_of_defaults,regular_defaulter,odd_defaulter,last_month_defaulter,\n",
        "                                            last_two_month_defaulter,num_of_defaults_in_last_two_month,last_three_month_defaulter,num_of_defaults_in_last_three_month,\n",
        "                                            num_of_deliquency_till_m13,InterestPending,ProbabilityToDefaultInWhole,ProbabilityToDefaultInSixMon,\n",
        "                                            ProbabilityToDefaultInThreeMon,EMI,EMIPerBorrower,UnpaidPrincipalNotInsured,LoanSize,Potential,borrower_credit_score_number),axis=1)\n",
        "  \n",
        "  ################# OneHotEncodeDataset #########################\n",
        "  regular_defaulterOHE=OneHotEncodeDataset(regular_defaulter)\n",
        "  odd_defaulterOHE=OneHotEncodeDataset(odd_defaulter)\n",
        "  last_month_defaulterOHE=OneHotEncodeDataset(last_month_defaulter)\n",
        "  last_two_month_defaulterOHE=OneHotEncodeDataset(last_two_month_defaulter)\n",
        "  last_three_month_defaulterOHE=OneHotEncodeDataset(last_three_month_defaulter)\n",
        "  insurance_typeOHE=OneHotEncodeDataset(insurance_type)\n",
        "  co_borrower_credit_scoreOHE=OneHotEncodeDataset(co_borrower_credit_score)\n",
        "  loan_purposeOHE=OneHotEncodeDataset(loan_purpose)\n",
        "  borrower_credit_scoreOHE=OneHotEncodeDataset(borrower_credit_score)\n",
        "  OriginMonthOHE=OneHotEncodeDataset(OriginMonth)\n",
        "  PaymentMonthOHE=OneHotEncodeDataset(PaymentMonth)\n",
        "  IntervalOHE=OneHotEncodeDataset(Interval)\n",
        "  RecurrenceOHE=OneHotEncodeDataset(Recurrence)\n",
        "  financial_institutionOHE=OneHotEncodeDataset(financial_institution)\n",
        "  sourceOHE=OneHotEncodeDataset(source)\n",
        "  \"\"\"\n",
        "  print(\"sourceOHE: \"+str(sourceOHE.shape[1]))\n",
        "  print(\"financial_institutionOHE: \"+str(financial_institutionOHE.shape[1]))\n",
        "  print(\"interest_rate: \"+str(interest_rate.shape[1]))\n",
        "  print(\"UnpaidPrincipal: \"+str(UnpaidPrincipal.shape[1]))\n",
        "  print(\"LoanTerm: \"+str(LoanTerm.shape[1]))\n",
        "  print(\"PtoTRatio: \"+str(PtoTRatio.shape[1]))\n",
        "  print(\"OriginMonthOHE: \"+str(OriginMonthOHE.shape[1]))\n",
        "  print(\"PaymentMonthOHE: \"+str(PaymentMonthOHE.shape[1]))\n",
        "  print(\"IntervalOHE: \"+str(IntervalOHE.shape[1]))\n",
        "  print(\"RecurrenceOHE: \"+str(RecurrenceOHE.shape[1]))\n",
        "  print(\"loan_to_value: \"+str(loan_to_value.shape[1]))\n",
        "  print(\"num_of_borrower: \"+str(num_of_borrower.shape[1]))\n",
        "  print(\"debt_to_income_ratio: \"+str(debt_to_income_ratio.shape[1]))\n",
        "  print(\"borrower_credit_scoreOHE: \"+str(borrower_credit_scoreOHE.shape[1]))\n",
        "  print(\"loan_purposeOHE: \"+str(loan_purposeOHE.shape[1]))\n",
        "  print(\"insurance_percent: \"+str(insurance_percent.shape[1]))\n",
        "  print(\"co_borrower_credit_scoreOHE: \"+str(co_borrower_credit_scoreOHE.shape[1]))\n",
        "  print(\"insurance_typeOHE: \"+str(insurance_typeOHE.shape[1]))\n",
        "  print(\"m1: \"+str(m1.shape[1]))\n",
        "  print(\"m2: \"+str(m2.shape[1]))\n",
        "  print(\"m3: \"+str(m3.shape[1]))\n",
        "  print(\"m4: \"+str(m4.shape[1]))\n",
        "  print(\"m5: \"+str(m5.shape[1]))\n",
        "  print(\"m6: \"+str(m6.shape[1]))\n",
        "  print(\"m7: \"+str(m7.shape[1]))\n",
        "  print(\"m8: \"+str(m8.shape[1]))\n",
        "  print(\"m9: \"+str(m9.shape[1]))\n",
        "  print(\"m10: \"+str(m10.shape[1]))\n",
        "  print(\"m11: \"+str(m11.shape[1]))\n",
        "  print(\"m12: \"+str(m12.shape[1]))\n",
        "  print(\"sum_of_defaults: \"+str(sum_of_defaults.shape[1]))\n",
        "  print(\"regular_defaulterOHE: \"+str(regular_defaulterOHE.shape[1]))\n",
        "  print(\"odd_defaulterOHE: \"+str(odd_defaulterOHE.shape[1]))\n",
        "  print(\"last_month_defaulterOHE: \"+str(last_month_defaulterOHE.shape[1]))\n",
        "  print(\"last_two_month_defaulterOHE: \"+str(last_two_month_defaulterOHE.shape[1]))\n",
        "  print(\"num_of_defaults_in_last_two_month: \"+str(num_of_defaults_in_last_two_month.shape[1]))\n",
        "  print(\"last_three_month_defaulterOHE: \"+str(last_three_month_defaulterOHE.shape[1]))\n",
        "  print(\"num_of_defaults_in_last_three_month: \"+str(num_of_defaults_in_last_three_month.shape[1]))\n",
        "  \"\"\"\n",
        "  \n",
        "  #################### Concat Columns ############################\n",
        "  DatasetPreProcessed= np.concatenate((sourceOHE,financial_institutionOHE,interest_rate,UnpaidPrincipal,LoanTerm,PtoTRatio,OriginMonthOHE,PaymentMonthOHE,IntervalOHE,RecurrenceOHE,\n",
        "                                       loan_to_value,num_of_borrower,debt_to_income_ratio,borrower_credit_scoreOHE,loan_purposeOHE,insurance_percent,co_borrower_credit_scoreOHE,\n",
        "                                       insurance_typeOHE,m1,m2,m3,m4,m5,m6,m7,m8,m9,m10,m11,m12,sum_of_defaults,regular_defaulterOHE,odd_defaulterOHE,last_month_defaulterOHE,\n",
        "                                       last_two_month_defaulterOHE,num_of_defaults_in_last_two_month,last_three_month_defaulterOHE,num_of_defaults_in_last_three_month,\n",
        "                                       num_of_deliquency_till_m13,InterestPending,ProbabilityToDefaultInWhole,ProbabilityToDefaultInSixMon,\n",
        "                                       ProbabilityToDefaultInThreeMon,EMI,EMIPerBorrower,UnpaidPrincipalNotInsured,LoanSize,Potential,borrower_credit_score_number),axis=1)\n",
        "  \n",
        "  \n",
        "  return DatasetPreProcessed,DatasetPreProcessedNoOHE\n",
        "\n",
        "## 9. Scale Dataset\n",
        "def ScaleDataset(PreprocessedDataset):\n",
        "  robust_scaler=RobustScaler(quantile_range=(5,95))\n",
        "  scaler=StandardScaler()\n",
        "  PreprocessedDatasetScaled=robust_scaler.fit_transform(PreprocessedDataset)\n",
        "  return PreprocessedDatasetScaled\n",
        "  \n",
        "  \n",
        "#####################################################################################################################################################################\n",
        "\n",
        "###################################################################### Steps ########################################################################################\n",
        "## 1. Import Dataset\n",
        "TrainingDataset=pd.read_csv(TrainingDatasetPath)\n",
        "TestDataset=pd.read_csv(TestDatasetPath)\n",
        "XTrain=TrainingDataset.iloc[:,1:28].values\n",
        "num_of_rows_in_training_set=XTrain.shape[0]\n",
        "YTrain=TrainingDataset.iloc[:,28].values\n",
        "XTest=TestDataset.iloc[:,1:28].values\n",
        "\n",
        "XTrain,XTest=FormatDate(XTrain,XTest)\n",
        "\n",
        "XTrainXTestConcatenated=np.concatenate((XTrain,XTest), axis=0)\n",
        "\n",
        "## 2. Deal with Invalid Values\n",
        "XTrainXTestImputed=DealWithMissingValues(XTrainXTestConcatenated)\n",
        "\n",
        "## 3. Preprocess Data\n",
        "XTrainXTestPreProcessed,DatasetPreProcessedNoOHE=PreprocessDataset(XTrainXTestImputed)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "0-1 source || 2-19 financial institution || 20 interest rate || 21 unpaid principal || 22 loan term || 23 prinicipal to term ratio || 24 - 25: origin month ||\n",
        "\n",
        "26-28 pay month || 29-32 interval || 33-34 recurrence || 35 loan to value ratio || 36 num of borrowers || 37 debt to income || 38-41 borrower credit score ||\n",
        "\n",
        "42-43 loan purpose || 44 insurance percent || 45-48 co borrower credit score || 49 insurance type || 50-61 m1 to m12 || 62 sum of defaults || 63 regular defaulter ||\n",
        "\n",
        "64 odd defaulter || 65 last month defaulter || 66 last 2 month defaulter || 67 num of defaults in last 2 month || 68 last 3 month defaulter || 69 num of defaults in last 3 month ||\n",
        "\n",
        "70 num_of_deliquency_till_m13 || 71 Interest Pending || 72 ProbabilityToDefaultInWhole || 73 ProbabilityToDefaultInSixMon || 74 ProbabilityToDefaultInThreeMon || \n",
        "\n",
        "75 EMI || 76 EMIPerBorrower || 77 UnpaidPrincipalNotInsured || 78 LoanSize || 79 Potential || 80 borrower_credit_score_number\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "## 4. Scale Dataset\n",
        "XTrainXTestScaled=ScaleDataset(XTrainXTestPreProcessed)\n",
        "\n",
        "## 5. Divide Dataset to Train and Test\n",
        "XTrainScaled=XTrainXTestScaled[0:num_of_rows_in_training_set,:]\n",
        "XTestScaled=XTrainXTestScaled[num_of_rows_in_training_set:XTrainXTestScaled.shape[0],:]\n",
        "\n",
        "## 6. Feature Importance\n",
        "\"\"\"\n",
        "np.savetxt(fname=\"WholeTrainDataset.csv\",X=XTrainScaled,fmt=\"%s\",delimiter=',')\n",
        "#After downloading give name to column names and reupload, similarly do for labels\n",
        "np.savetxt(fname=\"TrainSetLabel.csv\",X=YTrain.reshape(-1,1),fmt=\"%s\",delimiter=',')\n",
        "#After downloading give name to column name as Label and reupload\n",
        "#Now read the datasets with columns back \n",
        "dataset=pd.read_csv('WholeTrainDataset.csv')\n",
        "Label=pd.read_csv('TrainSetLabel.csv')\n",
        "from FeatureSelector import FeatureSelector\n",
        "fs = FeatureSelector(data = dataset, labels = Label)\n",
        "fs.identify_zero_importance(task = 'classification', eval_metric = 'f1', n_iterations = 10, early_stopping = True)\n",
        "importance=fs.feature_importances.iloc[:,0:2].values\n",
        "importance\n",
        "# based on the above findings features with zero importances are FinInst2,FinInst10,FinInst13 and BorCredScore4 which has indexes 3, 11, 14, 41\n",
        "\"\"\"\n",
        "#selected_columns=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78]\n",
        "#selected_columns=[0,1,2,5,6,9,20,21,22,23,30,35,36,37,38,39,40,42,43,45,48,60,62,69,71,75,76,77]\n",
        "#selected_columns=[0,1,2,4,5,6,7,8,9,10,12,13,15,16,17,18,19,23,29,30,31,32,33,34,35,37,38,39,40,42,43,45,46,47,48,49,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78]\n",
        "#selected_columns=[0,1,2,5,6,7,9,13,17,18,19,23,30,34,35,37,38,39,40,42,43,45,46,48,61,62,67,69,70,71,72,73,75,76,77,78]\n",
        "#selected_columns=[0,1,2,5,6,7,9,10,14,17,18,19,20,21,22,23,26,28,35,36,37,38,39,40,42,43,45,47,48,51,53,57,61,62,66,67,69,70,71,72,73,75,76,77,78]\n",
        "#selected_columns=[0,1,2,5,6,7,9,10,14,17,18,19,20,21,22,23,26,35,36,37,38,39,40,42,43,45,47,48,53,62,67,69,70,71,72,73,75,76,77,78]\n",
        "selected_columns=[0,1,2,5,6,7,9,10,14,17,18,19,20,21,22,23,26,28,35,36,37,38,39,40,42,43,45,47,48,51,53,55,57,61,62,66,67,69,70,71,72,73,75,76,77,78,79,80]\n",
        "\n",
        "XTrainScaled=XTrainScaled[:,selected_columns]\n",
        "XTestScaled=XTestScaled[:,selected_columns]\n",
        "\n",
        "TestSetPrediction1=[]\n",
        "TestSetPrediction2=[]\n",
        "TestSetPrediction3=[]\n",
        "TestSetPrediction4=[]\n",
        "TestSetPrediction5=[]\n",
        "TestSetPrediction6=[]\n",
        "TestSetPrediction7=[]\n",
        "TestSetPrediction8=[]\n",
        "TestSetPrediction9=[]\n",
        "TestSetPrediction10=[]\n",
        "\n",
        "XTrainScaled_Train, XTrainScaled_Test, YTrain_Train, YTrain_Test = train_test_split(XTrainScaled,YTrain,test_size=.10,shuffle=True,stratify=YTrain,random_state=1)\n",
        "\n",
        "for i in range(1,11):\n",
        "## 6. Train Test split\n",
        "  class_weight = {0:1,1:181.48}\n",
        "\n",
        "  print(\"################################################### Starting Iteration \"+str(i)+\" ##################################################################\")  \n",
        "  \n",
        "  ClassifierANN=Sequential()\n",
        "  ClassifierANN.add(Dense(input_shape=(48,),units=800,activation='relu',kernel_initializer='random_uniform'))\n",
        "  ClassifierANN.add(Dropout(0.5))\n",
        "  ClassifierANN.add(Dense(units=800,activation='relu',kernel_initializer='random_uniform'))\n",
        "  ClassifierANN.add(Dropout(0.5))\n",
        "  ClassifierANN.add(Dense(units=800,activation='relu',kernel_initializer='random_uniform'))\n",
        "  ClassifierANN.add(Dropout(0.5))\n",
        "  ClassifierANN.add(Dense(units=1,activation='sigmoid',kernel_initializer='random_uniform'))\n",
        "  ClassifierANN.compile(optimizer='adam',loss='binary_crossentropy',metrics=[f1_m])\n",
        "  ClassifierANN.fit(x=XTrainScaled_Train,y=YTrain_Train,batch_size=96,epochs=50,validation_data=(XTrainScaled_Test,YTrain_Test),callbacks=[EarlyStopping],class_weight=class_weight)\n",
        "  YPred=ClassifierANN.predict(XTestScaled)\n",
        "  YPred[YPred>0.5]=1\n",
        "  YPred[YPred<=0.5]=0\n",
        "  if i==1:\n",
        "    TestSetPrediction1=YPred\n",
        "  elif i==2:\n",
        "    TestSetPrediction2=YPred\n",
        "  elif i==3:\n",
        "    TestSetPrediction3=YPred\n",
        "  elif i==4:\n",
        "    TestSetPrediction4=YPred\n",
        "  elif i==5:\n",
        "    TestSetPrediction5=YPred\n",
        "  elif i==6:\n",
        "    TestSetPrediction6=YPred\n",
        "  elif i==7:\n",
        "    TestSetPrediction7=YPred\n",
        "  elif i==8:\n",
        "    TestSetPrediction8=YPred\n",
        "  elif i==9:\n",
        "    TestSetPrediction9=YPred\n",
        "  else:\n",
        "    TestSetPrediction10=YPred\n",
        "  K.clear_session()\n",
        "  print(\"\")\n",
        "  print(\"################################################### Ending Iteration \"+str(i)+\" ##################################################################\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QaDM9eDLFP3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##################################################################### Test Set Prediction ################################################################################################\n",
        "for count in range(0,len(TestSetPrediction1)):\n",
        "  if (TestSetPrediction1[count]+TestSetPrediction2[count]+TestSetPrediction3[count]+TestSetPrediction4[count]+TestSetPrediction6[count]+\n",
        "      TestSetPrediction7[count]+TestSetPrediction8[count]+TestSetPrediction9[count]+TestSetPrediction10[count]) > 6:\n",
        "    FinalYPred.append(1)\n",
        "  else:\n",
        "    FinalYPred.append(0)\n",
        "np.savetxt(fname=\"TestSetPrediction.txt\",X=FinalYPred,fmt=\"%s\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}